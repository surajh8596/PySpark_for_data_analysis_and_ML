{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69cfc885",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue\">What is PySPark?</h1>\n",
    "<h3 style=\"color:green; line-height:1.4\">PySpark is the Python API for Apache Spark, a fast and general-purpose cluster computing system. Apache Spark is designed to process large amounts of data in a distributed and parallel fashion across a cluster of computers. It offers high-level APIs in various languages, including Scala, Java, Python (PySpark), and R. PySpark allows you to write Spark applications using the Python programming language, making it more accessible to Python developers who may not be familiar with Scala or Java. It provides an interface to interact with Spark's core components, such as Resilient Distributed Datasets (RDDs) and DataFrames, as well as libraries for machine learning (MLlib), SQL querying (Spark SQL), graph processing (GraphX), and streaming (Structured Streaming).`</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1567e",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue\">Install the PySpark library and Import</h1>\n",
    "<h3 style=\"color:green; line-height:1.4\">To install PySpark library use code <code>pip install pyspark</code>in your command promt or anaconda promt or in jupyter notebook cell. To import use code <code>import pyspark.</code></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a10ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b29e39b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af7d77",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue\">List of attributes and functions defined in the <code>pyspark</code> module</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb90347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accumulator',\n",
       " 'AccumulatorParam',\n",
       " 'Any',\n",
       " 'BarrierTaskContext',\n",
       " 'BarrierTaskInfo',\n",
       " 'BasicProfiler',\n",
       " 'Broadcast',\n",
       " 'CPickleSerializer',\n",
       " 'Callable',\n",
       " 'HiveContext',\n",
       " 'InheritableThread',\n",
       " 'MarshalSerializer',\n",
       " 'Optional',\n",
       " 'Profiler',\n",
       " 'RDD',\n",
       " 'RDDBarrier',\n",
       " 'Row',\n",
       " 'SQLContext',\n",
       " 'SparkConf',\n",
       " 'SparkContext',\n",
       " 'SparkFiles',\n",
       " 'SparkJobInfo',\n",
       " 'SparkStageInfo',\n",
       " 'StatusTracker',\n",
       " 'StorageLevel',\n",
       " 'TaskContext',\n",
       " 'TypeVar',\n",
       " 'Union',\n",
       " '_F',\n",
       " '_NoValue',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_globals',\n",
       " 'accumulators',\n",
       " 'broadcast',\n",
       " 'cast',\n",
       " 'cloudpickle',\n",
       " 'conf',\n",
       " 'context',\n",
       " 'copy_func',\n",
       " 'errors',\n",
       " 'files',\n",
       " 'filterwarnings',\n",
       " 'find_spark_home',\n",
       " 'inheritable_thread_target',\n",
       " 'java_gateway',\n",
       " 'join',\n",
       " 'keyword_only',\n",
       " 'profiler',\n",
       " 'rdd',\n",
       " 'rddsampler',\n",
       " 'resource',\n",
       " 'resultiterable',\n",
       " 'serializers',\n",
       " 'shuffle',\n",
       " 'since',\n",
       " 'sql',\n",
       " 'statcounter',\n",
       " 'status',\n",
       " 'storagelevel',\n",
       " 'taskcontext',\n",
       " 'traceback_utils',\n",
       " 'types',\n",
       " 'util',\n",
       " 'version',\n",
       " 'wraps']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd522c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
